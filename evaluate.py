# -*- coding: utf-8 -*-
"""compare_AML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P-W8ogEFKDp2pbV0h71xM1gb6RhC0xFG

# Load UNIFIEDQA_MH
"""

import pickle
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer, util

class UnifiedQAWithRetrieval:
    def __init__(self, model, tokenizer, embed_model):
        self.model = model
        self.tokenizer = tokenizer
        self.embed_model = embed_model

    @staticmethod
    def extract_after_first_newline(text: str):
        text = text.replace("\\n", "\n")
        parts = text.split("\n", 1)
        question = parts[0].strip()
        if len(parts) > 1:
            context = parts[1].strip() 
        else:
            context = ""
            print("Text does not contain a newline character.", text)
        return question, context

    def retrieve_relevant_chunk(self, question, context, max_tokens=500):
        paragraphs = [p.strip() for p in context.split("\n") if p.strip()]
        if not paragraphs:
            return ""
        paragraph_embeddings = self.embed_model.encode(paragraphs, convert_to_tensor=True)
        question_embedding = self.embed_model.encode(question, convert_to_tensor=True)

        similarities = util.cos_sim(question_embedding, paragraph_embeddings)[0]
        ranked_idx = similarities.argsort(descending=True)

        selected_text = ""
        token_count = 0
        for idx in ranked_idx:
            para = paragraphs[idx]
            para_tokens = len(para.split())
            if token_count + para_tokens <= max_tokens:
                selected_text += para + " "
                token_count += para_tokens
            if token_count >= max_tokens:
                break

        return selected_text.strip()

    def answer_question(self, input_text, **generate_kwargs):
        question, context = self.extract_after_first_newline(input_text)
        retrieved_context = self.retrieve_relevant_chunk(question, context)
        final_input = question + " \n " + retrieved_context

        inputs = self.tokenizer(final_input, return_tensors="pt", truncation=True, padding=True)
        output_ids = self.model.generate(**inputs, **generate_kwargs)
        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)


# Load the QA system
with open("unifiedqa_with_retrieval.pkl", "rb") as f:
    unifiedqa_mh = pickle.load(f)
    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # unifiedqa_mh.model = unifiedqa_mh.model.to(device)

"""# Load UNIFIEDQA"""

unifiedqa_model_id = "allenai/unifiedqa-t5-small"
unifiedqa_tokenizer = AutoTokenizer.from_pretrained(unifiedqa_model_id)
unifiedqa_model = AutoModelForSeq2SeqLM.from_pretrained(unifiedqa_model_id)

# unifiedqa_model = unifiedqa_model.to(device)

def run_model_unifiedqa(input_string, **generator_args):
    input_ids = unifiedqa_tokenizer.encode(input_string, return_tensors="pt")
    res = unifiedqa_model.generate(input_ids, **generator_args)
    return unifiedqa_tokenizer.batch_decode(res, skip_special_tokens=True)

"""# Supporting functions for evaluation"""

import collections
import string
import re
from difflib import SequenceMatcher
import pandas as pd

def normalize_answer(s):
    """Lower text, remove punctuation, articles, extra whitespace, normalize numbers."""
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)
    def white_space_fix(text):
        return ' '.join(text.split())
    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)
    def lower(text):
        return text.lower()
    def normalize_numbers(text):
        return re.sub(r'(?<=\d),(?=\d)', '', text)
    
    return white_space_fix(normalize_numbers(remove_articles(remove_punc(lower(s)))))

def get_tokens(s):
    if not s:
        return []
    return normalize_answer(s).split()

def compute_exact(a_gold, a_pred):
    return int(normalize_answer(a_gold) == normalize_answer(a_pred))

def compute_f1(a_gold, a_pred):
    gold_toks = get_tokens(a_gold)
    pred_toks = get_tokens(a_pred)
    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)
    num_same = sum(common.values())
    if len(gold_toks) == 0 or len(pred_toks) == 0:
        return int(gold_toks == pred_toks)
    if num_same == 0:
        return 0
    precision = num_same / len(pred_toks)
    recall = num_same / len(gold_toks)
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    return f1

def clean_prediction(answer):
    if isinstance(answer, list):
        answer = " ".join(answer)
    answer = answer.strip().replace('"', '').replace("'", "")
    answer = re.sub(r'\b(\w+(?: \w+)*)(: \1)+', r'\1', answer)
    answer = answer.strip(': ')
    return answer

def fuzzy_match(a_gold, a_pred, threshold=0.70):
    """Returns True if similarity > threshold."""
    return SequenceMatcher(None, a_gold.lower(), a_pred.lower()).ratio() > threshold

def compute_metrics_with_fuzzy(expected, predicted, fuzzy_threshold=0.70):
    """
    Compute both EM and F1 with fuzzy matching consideration.
    If fuzzy match is above threshold, use the maximum of calculated metric and a boosted value.
    """
    # Compute base metrics
    base_em = compute_exact(expected, predicted)
    base_f1 = compute_f1(expected, predicted)
    
    # Check fuzzy match
    is_fuzzy_match = fuzzy_match(expected, predicted, fuzzy_threshold)
    
    # Apply fuzzy matching boost
    if is_fuzzy_match:
        # For EM: if fuzzy match, treat as correct (1)
        final_em = 1
        # For F1: take the maximum of calculated F1 and a boosted value (0.9)
        # This gives some credit for close matches while preserving token-level accuracy
        final_f1 = max(base_f1, 0.9)
    else:
        final_em = base_em
        final_f1 = base_f1
    
    return final_em, final_f1, base_em, base_f1, is_fuzzy_match

def load_tsv(name):
    path = f"evaluation_datasets/{name}.tsv"
    try:
        df = pd.read_csv(path, sep="\t", header=None)
        print(f"Loaded {name}.tsv with shape {df.shape}")
        return df
    except FileNotFoundError:
        print(f"File not found: {path}")
        return None

# Dataset list
data = ["data_boolq_test", "data_arc_easy_test", "data_arc_hard_test", 
        "data_commonsenseqa_test", "data_squad2_test", "hotpot_test"]

# Evaluation results
results = []

for dataset in data:
    df = load_tsv(dataset)
    if df is None:
        continue
        
    n_samples = min(1000, len(df))
    df = df.sample(n=n_samples, random_state=42).reset_index(drop=True)

    # Initialize counters
    exact_match = 0
    f1_total = 0
    base_exact_match = 0  # Without fuzzy
    base_f1_total = 0     # Without fuzzy
    fuzzy_matches = 0
    total = len(df)
    
    print(f"\nEvaluating {dataset}...")
    
    # Iterate over rows
    for idx, row in df.iterrows():
        question = row[0]
        expected_answer = str(row[1])  # Ensure string type

        # Get model prediction
        answer = unifiedqa_mh.answer_question(question)
        answer = clean_prediction(answer.strip())

        # Compute metrics with fuzzy matching
        em, f1, base_em, base_f1, is_fuzzy = compute_metrics_with_fuzzy(
            expected_answer, answer, fuzzy_threshold=0.70
        )
        
        exact_match += em
        f1_total += f1
        base_exact_match += base_em
        base_f1_total += base_f1
        fuzzy_matches += 1 if is_fuzzy else 0

        # Log mismatches (only for non-fuzzy, non-exact cases)
        if em == 0 and not is_fuzzy:
            print(f"Mismatch at row {idx}")
            print("Question :", question)
            print("Predicted:", answer)
            print("Target   :", expected_answer)
            print("---")

    # Calculate percentages
    exact_match_percent = 100.0 * exact_match / total
    f1_percent = 100.0 * f1_total / total
    base_exact_match_percent = 100.0 * base_exact_match / total
    base_f1_percent = 100.0 * base_f1_total / total
    fuzzy_match_percent = 100.0 * fuzzy_matches / total

    print(f"\n=== Results for {dataset} ===")
    print(f"Base EM (no fuzzy)  : {base_exact_match_percent:.2f}%")
    print(f"Base F1 (no fuzzy)  : {base_f1_percent:.2f}%")
    print(f"Fuzzy matches       : {fuzzy_match_percent:.2f}%")
    print(f"Final EM (w/ fuzzy) : {exact_match_percent:.2f}%")
    print(f"Final F1 (w/ fuzzy) : {f1_percent:.2f}%")
    print(f"Samples evaluated   : {total}")

    # Store results
    results.append({
        "dataset": dataset,
        "EM_base": base_exact_match_percent,
        "F1_base": base_f1_percent,
        "Fuzzy_matches": fuzzy_match_percent,
        "EM_final": exact_match_percent,
        "F1_final": f1_percent,
        "samples": total
    })

# Create summary dataframe
results_df = pd.DataFrame(results)
print("\n" + "="*60)
print("SUMMARY OF RESULTS")
print("="*60)
print(results_df.to_string(index=False))

# Save results to file
results_df.to_csv("eval_results_unifiedqa_mh.csv", index=False)
print("Saved results to eval_results_unifiedqa_mh.csv")

"""# Evaluate UNIFIEDQA"""

results = []

for dataset in data:
    df = load_tsv(dataset)
    if df is None:
        continue
        
    n_samples = min(1000, len(df))
    df = df.sample(n=n_samples, random_state=42).reset_index(drop=True)

    # Initialize counters
    exact_match = 0
    f1_total = 0
    base_exact_match = 0  # Without fuzzy
    base_f1_total = 0     # Without fuzzy
    fuzzy_matches = 0
    total = len(df)
    
    print(f"\nEvaluating {dataset}...")
    
    # Iterate over rows
    for idx, row in df.iterrows():
        question = row[0]
        expected_answer = str(row[1])  # Ensure string type

        # Get model prediction
        answer = run_model_unifiedqa(question)
        answer = " ".join([str(a) for a in answer])
        answer = clean_prediction(answer.strip())

        # Compute metrics with fuzzy matching
        em, f1, base_em, base_f1, is_fuzzy = compute_metrics_with_fuzzy(
            expected_answer, answer, fuzzy_threshold=0.70
        )
        
        exact_match += em
        f1_total += f1
        base_exact_match += base_em
        base_f1_total += base_f1
        fuzzy_matches += 1 if is_fuzzy else 0

        # Log mismatches (only for non-fuzzy, non-exact cases)
        if em == 0 and not is_fuzzy:
            print(f"Mismatch at row {idx}")
            print("Question :", question)
            print("Predicted:", answer)
            print("Target   :", expected_answer)
            print("---")

    # Calculate percentages
    exact_match_percent = 100.0 * exact_match / total
    f1_percent = 100.0 * f1_total / total
    base_exact_match_percent = 100.0 * base_exact_match / total
    base_f1_percent = 100.0 * base_f1_total / total
    fuzzy_match_percent = 100.0 * fuzzy_matches / total

    print(f"\n=== Results for {dataset} ===")
    print(f"Base EM (no fuzzy)  : {base_exact_match_percent:.2f}%")
    print(f"Base F1 (no fuzzy)  : {base_f1_percent:.2f}%")
    print(f"Fuzzy matches       : {fuzzy_match_percent:.2f}%")
    print(f"Final EM (w/ fuzzy) : {exact_match_percent:.2f}%")
    print(f"Final F1 (w/ fuzzy) : {f1_percent:.2f}%")
    print(f"Samples evaluated   : {total}")

    # Store results
    results.append({
        "dataset": dataset,
        "EM_base": base_exact_match_percent,
        "F1_base": base_f1_percent,
        "Fuzzy_matches": fuzzy_match_percent,
        "EM_final": exact_match_percent,
        "F1_final": f1_percent,
        "samples": total
    })

# Create summary dataframe
results_df = pd.DataFrame(results)
print("\n" + "="*60)
print("UNIFIEDQA - SUMMARY OF RESULTS")
print("="*60)
print(results_df.to_string(index=False))

# Save results to file
results_df.to_csv("eval_results_unifiedqa.csv", index=False)
print("Saved results to eval_results_unifiedqa.csv")