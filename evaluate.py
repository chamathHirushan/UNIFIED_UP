# -*- coding: utf-8 -*-
"""compare_AML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P-W8ogEFKDp2pbV0h71xM1gb6RhC0xFG

# Load UNIFIEDQA_MH
"""

import pickle
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer, util

class UnifiedQAWithRetrieval:
    def __init__(self, model, tokenizer, embed_model):
        self.model = model
        self.tokenizer = tokenizer
        self.embed_model = embed_model

    @staticmethod
    def extract_after_first_newline(text: str):
        parts = text.split("\n", 1)
        return parts[0].strip(), parts[1].strip()

    def retrieve_relevant_chunk(self, question, context, max_tokens=500):
        paragraphs = [p.strip() for p in context.split("\n") if p.strip()]
        paragraph_embeddings = self.embed_model.encode(paragraphs, convert_to_tensor=True)
        question_embedding = self.embed_model.encode(question, convert_to_tensor=True)

        similarities = util.cos_sim(question_embedding, paragraph_embeddings)[0]
        ranked_idx = similarities.argsort(descending=True)

        selected_text = ""
        token_count = 0
        for idx in ranked_idx:
            para = paragraphs[idx]
            para_tokens = len(para.split())
            if token_count + para_tokens <= max_tokens:
                selected_text += para + " "
                token_count += para_tokens
            if token_count >= max_tokens:
                break

        return selected_text.strip()

    def answer_question(self, input_text, **generate_kwargs):
        question, context = self.extract_after_first_newline(input_text)
        retrieved_context = self.retrieve_relevant_chunk(question, context)
        final_input = question + " \n " + retrieved_context

        inputs = self.tokenizer(final_input, return_tensors="pt", truncation=True, padding=True)
        output_ids = self.model.generate(**inputs, **generate_kwargs)
        return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)


# Load the QA system
with open("unifiedqa_with_retrieval.pkl", "rb") as f:
    unifiedqa_mh = pickle.load(f)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    unifiedqa_mh.model = unifiedqa_mh.model.to(device)

"""# Load UNIFIEDQA"""

unifiedqa_model_id = "allenai/unifiedqa-t5-small"
unifiedqa_tokenizer = AutoTokenizer.from_pretrained(unifiedqa_model_id)
unifiedqa_model = AutoModelForSeq2SeqLM.from_pretrained(unifiedqa_model_id)

unifiedqa_model = unifiedqa_model.to(device)

def run_model_unifiedqa(input_string, **generator_args):
    input_ids = unifiedqa_tokenizer.encode(input_string, return_tensors="pt")
    res = unifiedqa_model.generate(input_ids, **generator_args)
    return unifiedqa_tokenizer.batch_decode(res, skip_special_tokens=True)

"""# Supporting functions for evaluation"""

import collections
import string
import re
from difflib import SequenceMatcher
import pandas as pd

def normalize_answer(s):
    """Lower text, remove punctuation, articles, extra whitespace, normalize numbers."""
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)
    def white_space_fix(text):
        return ' '.join(text.split())
    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)
    def lower(text):
        return text.lower()
    def normalize_numbers(text):
        text = re.sub(r'(?<=\d),(?=\d)', '', text)
        return text
    return white_space_fix(normalize_numbers(remove_articles(remove_punc(lower(s)))))

def get_tokens(s):
    if not s:
        return []
    return normalize_answer(s).split()

def compute_exact(a_gold, a_pred):
    return int(normalize_answer(a_gold) == normalize_answer(a_pred))

def compute_f1(a_gold, a_pred):
    gold_toks = get_tokens(a_gold)
    pred_toks = get_tokens(a_pred)
    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)
    num_same = sum(common.values())
    if len(gold_toks) == 0 or len(pred_toks) == 0:
        return int(gold_toks == pred_toks)
    if num_same == 0:
        return 0
    precision = num_same / len(pred_toks)
    recall = num_same / len(gold_toks)
    return 2 * precision * recall / (precision + recall)

def clean_prediction(answer):
    if isinstance(answer, list):
        answer = " ".join(answer)
    answer = answer.strip().replace('"', '').replace("'", "")
    answer = re.sub(r'\b(\w+(?: \w+)*)(: \1)+', r'\1', answer)
    answer = answer.strip(': ')
    return answer

def fuzzy_match(a_gold, a_pred, threshold=0.70):
    """Returns True if similarity > threshold."""
    return SequenceMatcher(None, a_gold.lower(), a_pred.lower()).ratio() > threshold

def load_tsv(name):
    path = f"/evaluation_datasets/{name}.tsv"
    try:
        df = pd.read_csv(path, sep="\t", header=None)
        print(f"Loaded {name}.tsv with shape {df.shape}")
        return df
    except FileNotFoundError:
        print(f"File not found: {path}")
        return None

data=["data_boolq_test","data_arc_easy_test","data_arc_hard_test","data_commonsenseqa_test","data_squad2_test","hotpot_text"]

"""# Evaluate UNIFIEDQA_MH"""

results = []

for dataset in data:
    df = load_tsv(dataset)
    n_samples = min(1000, len(df))
    df = df.sample(n=n_samples, random_state=42).reset_index(drop=True)
    if df is None:
        continue

    exact_match = 0
    f1_total = 0
    total = len(df)
    # Iterate over rows; assuming df has columns: question, label
    for idx, row in df.iterrows():
        question = row[0]
        expected_answer = row[1]

        answer = unifiedqa_mh.answer_question(question)
        answer = " ".join([str(a) for a in answer])

        # Compare answers
        em = compute_exact(expected_answer, answer)
        if em == 0 and fuzzy_match(expected_answer, answer):
          em = 1  # Treat as correct for EM

        f1 = compute_f1(expected_answer, answer)
        exact_match += em
        f1_total += f1

        if em == 0:
          print(f"Mismatch at row {idx}")
          print("Predicted:", answer)
          print("Target   :", expected_answer)

    # Final scores
    exact_match_percent = 100.0 * exact_match / total
    f1_percent = 100.0 * f1_total / total

    print(f"Exact Match: {exact_match_percent:.2f}%")
    print(f"F1 Score   : {f1_percent:.2f}%")

    results.append({
        "dataset": dataset,
        "EM": exact_match_percent,
        "F1": f1_percent
    })

# Save all results to a CSV
results_df = pd.DataFrame(results)
results_df.to_csv("eval_results_unifiedqa_mh.csv", index=False)
print("Saved results to eval_results_unifiedqa_mh.csv")

"""# Evaluate UNIFIEDQA"""

results = []

for dataset in data:
    df = load_tsv(dataset)
    n_samples = min(1000, len(df))
    df = df.sample(n=n_samples, random_state=42).reset_index(drop=True)
    if df is None:
        continue

    exact_match = 0
    f1_total = 0
    total = len(df)
    # Iterate over rows; assuming df has columns: question, label
    for idx, row in df.iterrows():
        question = row[0]
        expected_answer = row[1]

        answer = run_model_unifiedqa(question)
        answer = " ".join([str(a) for a in answer])

        # Compare answers
        em = compute_exact(expected_answer, answer)
        if em == 0 and fuzzy_match(expected_answer, answer):
          em = 1  # Treat as correct for EM

        f1 = compute_f1(expected_answer, answer)
        exact_match += em
        f1_total += f1

        if em == 0:
          print(f"Mismatch at row {idx}")
          print("Predicted:", answer)
          print("Target   :", expected_answer)

    # Final scores
    exact_match_percent = 100.0 * exact_match / total
    f1_percent = 100.0 * f1_total / total

    print(f"Exact Match: {exact_match_percent:.2f}%")
    print(f"F1 Score   : {f1_percent:.2f}%")

    results.append({
        "dataset": dataset,
        "EM": exact_match_percent,
        "F1": f1_percent
    })

# Save all results to a CSV
results_df = pd.DataFrame(results)
results_df.to_csv("eval_results_unifiedqa.csv", index=False)
print("Saved results to eval_results_unifiedqa.csv")