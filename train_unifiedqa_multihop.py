# -*- coding: utf-8 -*-
"""train_unifiedQA_multihop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t1GToDwdic3cs3kOEVoRTJ0mm3c9Qdj8
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import load_dataset, DatasetDict, Dataset
import torch

model_name = "allenai/unifiedqa-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

dataset = load_dataset("hotpotqa/hotpot_qa", "distractor")

train_ds = dataset["train"]
df = train_ds.to_pandas().sample(n=20000, random_state=42).reset_index(drop=True)

def stringify_context(context_json):
    """
    Converts the HotpotQA context into a single string without internal quotes.
    """
    titles = context_json.get("title", [])
    sentences_list = context_json.get("sentences", [])
    parts = []
    for t, sents in zip(titles, sentences_list):
        # Remove inner quotes from each sentence
        clean_sents = [sent.replace('"', '').replace("'", "") for sent in sents]
        # Join all sentences for this title
        parts.append(f"{t}: " + " ".join(clean_sents))
    # Join all title blocks into a single string and wrap with one outer quote
    return '"' + " | ".join(parts)

df['context_str'] = df['context'].apply(stringify_context)
df['input_text'] = df.apply(
    lambda row: f"question: {row['question']}\ncontext: {row['context_str']}", axis=1
)

# Keep only input_text and target_text
preprocessed_df = df[['input_text']].copy()
preprocessed_df['target_text'] = df['answer']

preprocessed_df.head(1)

def run_model(input_string, **generator_args):
    input_ids = tokenizer.encode(input_string, return_tensors="pt")
    res = model.generate(input_ids, **generator_args)
    return tokenizer.batch_decode(res, skip_special_tokens=True)

run_model(preprocessed_df['input_text'][0])

preprocessed_df['input_text'][0]

def preprocess(example):
    model_inputs = tokenizer(example["input_text"], max_length=512, truncation=True, padding="max_length")
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["target_text"], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


dataset = Dataset.from_pandas(preprocessed_df)

dataset = dataset.train_test_split(test_size=0.1)
train_dataset = dataset["train"]
val_dataset = dataset["test"]

tokenized_train = train_dataset.map(preprocess)
tokenized_val = val_dataset.map(preprocess)

training_args = Seq2SeqTrainingArguments(
    output_dir="./unifiedqa_finetuned",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    predict_with_generate=True,
    save_total_limit=2,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

trainer.train()

save_dir = "./unifiedqa_hotpotqa_model"

trainer.save_model(save_dir)
tokenizer.save_pretrained(save_dir) 